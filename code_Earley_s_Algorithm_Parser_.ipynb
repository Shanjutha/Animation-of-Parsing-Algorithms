{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animating String Parsing with Earley's Algorithm Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This module uses NLTKs Incremental Earley Chart Parser to parse a string. The printTree takes a nltk.tree.Tree  and  produces a derivation tree. The Animate method is used to generate interactive animations of the parsing process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cell contains the NLTK's Earley Chart Parser that has been re-implemented by the \"NLTK Project\" as the old version that could be imported is depreciated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Sentence:\n",
      "John saw a dog\n",
      "['John', 'saw', 'a', 'dog']\n",
      "\n",
      "|.   John  .   saw   .    a    .   dog   .|\n",
      "Leaf Init Rule:\n",
      "|[---------]         .         .         .| [0:1] 'John'\n",
      "|.         [---------]         .         .| [1:2] 'saw'\n",
      "|.         .         [---------]         .| [2:3] 'a'\n",
      "|.         .         .         [---------]| [3:4] 'dog'\n",
      "Top Down Init Rule:\n",
      "|>         .         .         .         .| [0:0] S  -> * NP VP\n",
      "\n",
      "* Processing queue: 0 \n",
      "\n",
      "Predictor Rule:\n",
      "|>         .         .         .         .| [0:0] NP -> * NP PP\n",
      "|>         .         .         .         .| [0:0] NP -> * Det Noun\n",
      "|>         .         .         .         .| [0:0] NP -> * 'John'\n",
      "\n",
      "* Processing queue: 1 \n",
      "\n",
      "Scanner Rule:\n",
      "|[---------]         .         .         .| [0:1] NP -> 'John' *\n",
      "Completer Rule:\n",
      "|[--------->         .         .         .| [0:1] S  -> NP * VP\n",
      "|[--------->         .         .         .| [0:1] NP -> NP * PP\n",
      "Predictor Rule:\n",
      "|.         >         .         .         .| [1:1] VP -> * VP PP\n",
      "|.         >         .         .         .| [1:1] VP -> * Verb NP\n",
      "|.         >         .         .         .| [1:1] VP -> * Verb\n",
      "Predictor Rule:\n",
      "|.         >         .         .         .| [1:1] Verb -> * 'saw'\n",
      "\n",
      "* Processing queue: 2 \n",
      "\n",
      "Scanner Rule:\n",
      "|.         [---------]         .         .| [1:2] Verb -> 'saw' *\n",
      "Completer Rule:\n",
      "|.         [--------->         .         .| [1:2] VP -> Verb * NP\n",
      "|.         [---------]         .         .| [1:2] VP -> Verb *\n",
      "Completer Rule:\n",
      "|[-------------------]         .         .| [0:2] S  -> NP VP *\n",
      "|.         [--------->         .         .| [1:2] VP -> VP * PP\n",
      "Predictor Rule:\n",
      "|.         .         >         .         .| [2:2] NP -> * NP PP\n",
      "|.         .         >         .         .| [2:2] NP -> * Det Noun\n",
      "Predictor Rule:\n",
      "|.         .         >         .         .| [2:2] Det -> * 'a'\n",
      "\n",
      "* Processing queue: 3 \n",
      "\n",
      "Scanner Rule:\n",
      "|.         .         [---------]         .| [2:3] Det -> 'a' *\n",
      "Completer Rule:\n",
      "|.         .         [--------->         .| [2:3] NP -> Det * Noun\n",
      "Predictor Rule:\n",
      "|.         .         .         >         .| [3:3] Noun -> * 'dog'\n",
      "\n",
      "* Processing queue: 4 \n",
      "\n",
      "Scanner Rule:\n",
      "|.         .         .         [---------]| [3:4] Noun -> 'dog' *\n",
      "Completer Rule:\n",
      "|.         .         [-------------------]| [2:4] NP -> Det Noun *\n",
      "Completer Rule:\n",
      "|.         [-----------------------------]| [1:4] VP -> Verb NP *\n",
      "|.         .         [------------------->| [2:4] NP -> NP * PP\n",
      "Completer Rule:\n",
      "|[=======================================]| [0:4] S  -> NP VP *\n",
      "|.         [----------------------------->| [1:4] VP -> VP * PP\n",
      "(S (NP John) (VP (Verb saw) (NP (Det a) (Noun dog))))\n",
      "----------------\n",
      "(S (NP John) (VP (Verb saw) (NP (Det a) (Noun dog))))\n",
      "[Tree('S', [Tree('NP', ['John']), Tree('VP', [Tree('Verb', ['saw']), Tree('NP', [Tree('Det', ['a']), Tree('Noun', ['dog'])])])])]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Natural Language Toolkit: An Incremental Earley Chart Parser\n",
    "#\n",
    "# Copyright (C) 2001-2020 NLTK Project\n",
    "# Author: Peter Ljunglöf <peter.ljunglof@heatherleaf.se>\n",
    "#         Rob Speer <rspeer@mit.edu>\n",
    "#         Edward Loper <edloper@gmail.com>\n",
    "#         Steven Bird <stevenbird1@gmail.com>\n",
    "#         Jean Mark Gawron <gawron@mail.sdsu.edu>\n",
    "# URL: <http://nltk.org/>\n",
    "# For license information, see LICENSE.TXT\n",
    "\n",
    "\"\"\"\n",
    "Data classes and parser implementations for *incremental* chart\n",
    "parsers, which use dynamic programming to efficiently parse a text.\n",
    "A \"chart parser\" derives parse trees for a text by iteratively adding\n",
    "\\\"edges\\\" to a \\\"chart\\\".  Each \"edge\" represents a hypothesis about the tree\n",
    "structure for a subsequence of the text.  The \"chart\" is a\n",
    "\\\"blackboard\\\" for composing and combining these hypotheses.\n",
    "\n",
    "A parser is \"incremental\", if it guarantees that for all i, j where i < j,\n",
    "all edges ending at i are built before any edges ending at j.\n",
    "This is appealing for, say, speech recognizer hypothesis filtering.\n",
    "\n",
    "The main parser class is ``EarleyChartParser``, which is a top-down\n",
    "algorithm, originally formulated by Jay Earley (1970).\n",
    "\"\"\"\n",
    "from time import perf_counter\n",
    "\n",
    "from nltk.parse.chart import (\n",
    "    Chart,\n",
    "    ChartParser,\n",
    "    EdgeI,\n",
    "    LeafEdge,\n",
    "    LeafInitRule,\n",
    "    BottomUpPredictRule,\n",
    "    BottomUpPredictCombineRule,\n",
    "    TopDownInitRule,\n",
    "    SingleEdgeFundamentalRule,\n",
    "    EmptyPredictRule,\n",
    "    CachedTopDownPredictRule,\n",
    "    FilteredSingleEdgeFundamentalRule,\n",
    "    FilteredBottomUpPredictCombineRule,\n",
    ")\n",
    "from nltk.parse.featurechart import (\n",
    "    FeatureChart,\n",
    "    FeatureChartParser,\n",
    "    FeatureTopDownInitRule,\n",
    "    FeatureTopDownPredictRule,\n",
    "    FeatureEmptyPredictRule,\n",
    "    FeatureBottomUpPredictRule,\n",
    "    FeatureBottomUpPredictCombineRule,\n",
    "    FeatureSingleEdgeFundamentalRule,\n",
    ")\n",
    "\n",
    "# ////////////////////////////////////////////////////////////\n",
    "# Incremental Chart\n",
    "# ////////////////////////////////////////////////////////////\n",
    "\n",
    "\n",
    "class IncrementalChart(Chart):\n",
    "    def initialize(self):\n",
    "        # A sequence of edge lists contained in this chart.\n",
    "        self._edgelists = tuple([] for x in self._positions())\n",
    "\n",
    "        # The set of child pointer lists associated with each edge.\n",
    "        self._edge_to_cpls = {}\n",
    "\n",
    "        # Indexes mapping attribute values to lists of edges\n",
    "        # (used by select()).\n",
    "        self._indexes = {}\n",
    "\n",
    "\n",
    "    def edges(self):\n",
    "        return list(self.iteredges())\n",
    "\n",
    "\n",
    "    def iteredges(self):\n",
    "        return (edge for edgelist in self._edgelists for edge in edgelist)\n",
    "\n",
    "\n",
    "    def select(self, end, **restrictions):\n",
    "        edgelist = self._edgelists[end]\n",
    "\n",
    "        # If there are no restrictions, then return all edges.\n",
    "        if restrictions == {}:\n",
    "            return iter(edgelist)\n",
    "\n",
    "        # Find the index corresponding to the given restrictions.\n",
    "        restr_keys = sorted(restrictions.keys())\n",
    "        restr_keys = tuple(restr_keys)\n",
    "\n",
    "        # If it doesn't exist, then create it.\n",
    "        if restr_keys not in self._indexes:\n",
    "            self._add_index(restr_keys)\n",
    "\n",
    "        vals = tuple(restrictions[key] for key in restr_keys)\n",
    "        return iter(self._indexes[restr_keys][end].get(vals, []))\n",
    "\n",
    "\n",
    "    def _add_index(self, restr_keys):\n",
    "        # Make sure it's a valid index.\n",
    "        for key in restr_keys:\n",
    "            if not hasattr(EdgeI, key):\n",
    "                raise ValueError(\"Bad restriction: %s\" % key)\n",
    "\n",
    "        # Create the index.\n",
    "        index = self._indexes[restr_keys] = tuple({} for x in self._positions())\n",
    "\n",
    "        # Add all existing edges to the index.\n",
    "        for end, edgelist in enumerate(self._edgelists):\n",
    "            this_index = index[end]\n",
    "            for edge in edgelist:\n",
    "                vals = tuple(getattr(edge, key)() for key in restr_keys)\n",
    "                this_index.setdefault(vals, []).append(edge)\n",
    "\n",
    "    def _register_with_indexes(self, edge):\n",
    "        end = edge.end()\n",
    "        for (restr_keys, index) in self._indexes.items():\n",
    "            vals = tuple(getattr(edge, key)() for key in restr_keys)\n",
    "            index[end].setdefault(vals, []).append(edge)\n",
    "\n",
    "    def _append_edge(self, edge):\n",
    "        self._edgelists[edge.end()].append(edge)\n",
    "\n",
    "    def _positions(self):\n",
    "        return range(self.num_leaves() + 1)\n",
    "\n",
    "\n",
    "\n",
    "class FeatureIncrementalChart(IncrementalChart, FeatureChart):\n",
    "    def select(self, end, **restrictions):\n",
    "        edgelist = self._edgelists[end]\n",
    "\n",
    "        # If there are no restrictions, then return all edges.\n",
    "        if restrictions == {}:\n",
    "            return iter(edgelist)\n",
    "\n",
    "        # Find the index corresponding to the given restrictions.\n",
    "        restr_keys = sorted(restrictions.keys())\n",
    "        restr_keys = tuple(restr_keys)\n",
    "\n",
    "        # If it doesn't exist, then create it.\n",
    "        if restr_keys not in self._indexes:\n",
    "            self._add_index(restr_keys)\n",
    "\n",
    "        vals = tuple(\n",
    "            self._get_type_if_possible(restrictions[key]) for key in restr_keys\n",
    "        )\n",
    "        return iter(self._indexes[restr_keys][end].get(vals, []))\n",
    "\n",
    "\n",
    "    def _add_index(self, restr_keys):\n",
    "        # Make sure it's a valid index.\n",
    "        for key in restr_keys:\n",
    "            if not hasattr(EdgeI, key):\n",
    "                raise ValueError(\"Bad restriction: %s\" % key)\n",
    "\n",
    "        # Create the index.\n",
    "        index = self._indexes[restr_keys] = tuple({} for x in self._positions())\n",
    "\n",
    "        # Add all existing edges to the index.\n",
    "        for end, edgelist in enumerate(self._edgelists):\n",
    "            this_index = index[end]\n",
    "            for edge in edgelist:\n",
    "                vals = tuple(\n",
    "                    self._get_type_if_possible(getattr(edge, key)())\n",
    "                    for key in restr_keys\n",
    "                )\n",
    "                this_index.setdefault(vals, []).append(edge)\n",
    "\n",
    "    def _register_with_indexes(self, edge):\n",
    "        end = edge.end()\n",
    "        for (restr_keys, index) in self._indexes.items():\n",
    "            vals = tuple(\n",
    "                self._get_type_if_possible(getattr(edge, key)()) for key in restr_keys\n",
    "            )\n",
    "            index[end].setdefault(vals, []).append(edge)\n",
    "\n",
    "\n",
    "\n",
    "# ////////////////////////////////////////////////////////////\n",
    "# Incremental CFG Rules\n",
    "# ////////////////////////////////////////////////////////////\n",
    "\n",
    "\n",
    "class CompleteFundamentalRule(SingleEdgeFundamentalRule):\n",
    "    def _apply_incomplete(self, chart, grammar, left_edge):\n",
    "        end = left_edge.end()\n",
    "        # When the chart is incremental, we only have to look for\n",
    "        # empty complete edges here.\n",
    "        for right_edge in chart.select(\n",
    "            start=end, end=end, is_complete=True, lhs=left_edge.nextsym()\n",
    "        ):\n",
    "            new_edge = left_edge.move_dot_forward(right_edge.end())\n",
    "            if chart.insert_with_backpointer(new_edge, left_edge, right_edge):\n",
    "                yield new_edge\n",
    "\n",
    "\n",
    "\n",
    "class CompleterRule(CompleteFundamentalRule):\n",
    "    _fundamental_rule = CompleteFundamentalRule()\n",
    "\n",
    "    def apply(self, chart, grammar, edge):\n",
    "        if not isinstance(edge, LeafEdge):\n",
    "            for new_edge in self._fundamental_rule.apply(chart, grammar, edge):\n",
    "                yield new_edge\n",
    "\n",
    "\n",
    "\n",
    "class ScannerRule(CompleteFundamentalRule):\n",
    "    _fundamental_rule = CompleteFundamentalRule()\n",
    "\n",
    "    def apply(self, chart, grammar, edge):\n",
    "        if isinstance(edge, LeafEdge):\n",
    "            for new_edge in self._fundamental_rule.apply(chart, grammar, edge):\n",
    "                yield new_edge\n",
    "\n",
    "\n",
    "\n",
    "class PredictorRule(CachedTopDownPredictRule):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "class FilteredCompleteFundamentalRule(FilteredSingleEdgeFundamentalRule):\n",
    "    def apply(self, chart, grammar, edge):\n",
    "        # Since the Filtered rule only works for grammars without empty productions,\n",
    "        # we only have to bother with complete edges here.\n",
    "        if edge.is_complete():\n",
    "            for new_edge in self._apply_complete(chart, grammar, edge):\n",
    "                yield new_edge\n",
    "\n",
    "\n",
    "\n",
    "# ////////////////////////////////////////////////////////////\n",
    "# Incremental FCFG Rules\n",
    "# ////////////////////////////////////////////////////////////\n",
    "\n",
    "\n",
    "class FeatureCompleteFundamentalRule(FeatureSingleEdgeFundamentalRule):\n",
    "    def _apply_incomplete(self, chart, grammar, left_edge):\n",
    "        fr = self._fundamental_rule\n",
    "        end = left_edge.end()\n",
    "        # When the chart is incremental, we only have to look for\n",
    "        # empty complete edges here.\n",
    "        for right_edge in chart.select(\n",
    "            start=end, end=end, is_complete=True, lhs=left_edge.nextsym()\n",
    "        ):\n",
    "            for new_edge in fr.apply(chart, grammar, left_edge, right_edge):\n",
    "                yield new_edge\n",
    "\n",
    "\n",
    "\n",
    "class FeatureCompleterRule(CompleterRule):\n",
    "    _fundamental_rule = FeatureCompleteFundamentalRule()\n",
    "\n",
    "\n",
    "\n",
    "class FeatureScannerRule(ScannerRule):\n",
    "    _fundamental_rule = FeatureCompleteFundamentalRule()\n",
    "\n",
    "\n",
    "\n",
    "class FeaturePredictorRule(FeatureTopDownPredictRule):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "# ////////////////////////////////////////////////////////////\n",
    "# Incremental CFG Chart Parsers\n",
    "# ////////////////////////////////////////////////////////////\n",
    "\n",
    "EARLEY_STRATEGY = [\n",
    "    LeafInitRule(),\n",
    "    TopDownInitRule(),\n",
    "    CompleterRule(),\n",
    "    ScannerRule(),\n",
    "    PredictorRule(),\n",
    "]\n",
    "TD_INCREMENTAL_STRATEGY = [\n",
    "    LeafInitRule(),\n",
    "    TopDownInitRule(),\n",
    "    CachedTopDownPredictRule(),\n",
    "    CompleteFundamentalRule(),\n",
    "]\n",
    "BU_INCREMENTAL_STRATEGY = [\n",
    "    LeafInitRule(),\n",
    "    EmptyPredictRule(),\n",
    "    BottomUpPredictRule(),\n",
    "    CompleteFundamentalRule(),\n",
    "]\n",
    "BU_LC_INCREMENTAL_STRATEGY = [\n",
    "    LeafInitRule(),\n",
    "    EmptyPredictRule(),\n",
    "    BottomUpPredictCombineRule(),\n",
    "    CompleteFundamentalRule(),\n",
    "]\n",
    "\n",
    "LC_INCREMENTAL_STRATEGY = [\n",
    "    LeafInitRule(),\n",
    "    FilteredBottomUpPredictCombineRule(),\n",
    "    FilteredCompleteFundamentalRule(),\n",
    "]\n",
    "\n",
    "\n",
    "class IncrementalChartParser(ChartParser):\n",
    "    \"\"\"\n",
    "    An *incremental* chart parser implementing Jay Earley's\n",
    "    parsing algorithm:\n",
    "\n",
    "    | For each index end in [0, 1, ..., N]:\n",
    "    |   For each edge such that edge.end = end:\n",
    "    |     If edge is incomplete and edge.next is not a part of speech:\n",
    "    |       Apply PredictorRule to edge\n",
    "    |     If edge is incomplete and edge.next is a part of speech:\n",
    "    |       Apply ScannerRule to edge\n",
    "    |     If edge is complete:\n",
    "    |       Apply CompleterRule to edge\n",
    "    | Return any complete parses in the chart\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        grammar,\n",
    "        strategy=BU_LC_INCREMENTAL_STRATEGY,\n",
    "        trace=0,\n",
    "        trace_chart_width=50,\n",
    "        chart_class=IncrementalChart,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a new Earley chart parser, that uses ``grammar`` to\n",
    "        parse texts.\n",
    "\n",
    "        :type grammar: CFG\n",
    "        :param grammar: The grammar used to parse texts.\n",
    "        :type trace: int\n",
    "        :param trace: The level of tracing that should be used when\n",
    "            parsing a text.  ``0`` will generate no tracing output;\n",
    "            and higher numbers will produce more verbose tracing\n",
    "            output.\n",
    "        :type trace_chart_width: int\n",
    "        :param trace_chart_width: The default total width reserved for\n",
    "            the chart in trace output.  The remainder of each line will\n",
    "            be used to display edges.\n",
    "        :param chart_class: The class that should be used to create\n",
    "            the charts used by this parser.\n",
    "        \"\"\"\n",
    "        self._grammar = grammar\n",
    "        self._trace = trace\n",
    "        self._trace_chart_width = trace_chart_width\n",
    "        self._chart_class = chart_class\n",
    "\n",
    "        self._axioms = []\n",
    "        self._inference_rules = []\n",
    "        for rule in strategy:\n",
    "            if rule.NUM_EDGES == 0:\n",
    "                self._axioms.append(rule)\n",
    "            elif rule.NUM_EDGES == 1:\n",
    "                self._inference_rules.append(rule)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Incremental inference rules must have \" \"NUM_EDGES == 0 or 1\"\n",
    "                )\n",
    "\n",
    "    def chart_parse(self, tokens, trace=None):\n",
    "        if trace is None:\n",
    "            trace = self._trace\n",
    "        trace_new_edges = self._trace_new_edges\n",
    "\n",
    "        tokens = list(tokens)\n",
    "        self._grammar.check_coverage(tokens)\n",
    "        chart = self._chart_class(tokens)\n",
    "        grammar = self._grammar\n",
    "\n",
    "        # Width, for printing trace edges.\n",
    "        trace_edge_width = self._trace_chart_width // (chart.num_leaves() + 1)\n",
    "        if trace:\n",
    "            print(chart.pretty_format_leaves(trace_edge_width))\n",
    "\n",
    "        for axiom in self._axioms:\n",
    "            new_edges = list(axiom.apply(chart, grammar))\n",
    "            trace_new_edges(chart, axiom, new_edges, trace, trace_edge_width)\n",
    "\n",
    "        inference_rules = self._inference_rules\n",
    "        for end in range(chart.num_leaves() + 1):\n",
    "            if trace > 1:\n",
    "                print(\"\\n* Processing queue:\", end, \"\\n\")\n",
    "            agenda = list(chart.select(end=end))\n",
    "            while agenda:\n",
    "                edge = agenda.pop()\n",
    "                for rule in inference_rules:\n",
    "                    new_edges = list(rule.apply(chart, grammar, edge))\n",
    "                    trace_new_edges(chart, rule, new_edges, trace, trace_edge_width)\n",
    "                    for new_edge in new_edges:\n",
    "                        if new_edge.end() == end:\n",
    "                            agenda.append(new_edge)\n",
    "\n",
    "        return chart\n",
    "\n",
    "\n",
    "\n",
    "class EarleyChartParser(IncrementalChartParser):\n",
    "    def __init__(self, grammar, **parser_args):\n",
    "        IncrementalChartParser.__init__(self, grammar, EARLEY_STRATEGY, **parser_args)\n",
    "\n",
    "\n",
    "\n",
    "class IncrementalTopDownChartParser(IncrementalChartParser):\n",
    "    def __init__(self, grammar, **parser_args):\n",
    "        IncrementalChartParser.__init__(\n",
    "            self, grammar, TD_INCREMENTAL_STRATEGY, **parser_args\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class IncrementalBottomUpChartParser(IncrementalChartParser):\n",
    "    def __init__(self, grammar, **parser_args):\n",
    "        IncrementalChartParser.__init__(\n",
    "            self, grammar, BU_INCREMENTAL_STRATEGY, **parser_args\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class IncrementalBottomUpLeftCornerChartParser(IncrementalChartParser):\n",
    "    def __init__(self, grammar, **parser_args):\n",
    "        IncrementalChartParser.__init__(\n",
    "            self, grammar, BU_LC_INCREMENTAL_STRATEGY, **parser_args\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class IncrementalLeftCornerChartParser(IncrementalChartParser):\n",
    "    def __init__(self, grammar, **parser_args):\n",
    "        if not grammar.is_nonempty():\n",
    "            raise ValueError(\n",
    "                \"IncrementalLeftCornerParser only works for grammars \"\n",
    "                \"without empty productions.\"\n",
    "            )\n",
    "        IncrementalChartParser.__init__(\n",
    "            self, grammar, LC_INCREMENTAL_STRATEGY, **parser_args\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "# ////////////////////////////////////////////////////////////\n",
    "# Incremental FCFG Chart Parsers\n",
    "# ////////////////////////////////////////////////////////////\n",
    "\n",
    "EARLEY_FEATURE_STRATEGY = [\n",
    "    LeafInitRule(),\n",
    "    FeatureTopDownInitRule(),\n",
    "    FeatureCompleterRule(),\n",
    "    FeatureScannerRule(),\n",
    "    FeaturePredictorRule(),\n",
    "]\n",
    "TD_INCREMENTAL_FEATURE_STRATEGY = [\n",
    "    LeafInitRule(),\n",
    "    FeatureTopDownInitRule(),\n",
    "    FeatureTopDownPredictRule(),\n",
    "    FeatureCompleteFundamentalRule(),\n",
    "]\n",
    "BU_INCREMENTAL_FEATURE_STRATEGY = [\n",
    "    LeafInitRule(),\n",
    "    FeatureEmptyPredictRule(),\n",
    "    FeatureBottomUpPredictRule(),\n",
    "    FeatureCompleteFundamentalRule(),\n",
    "]\n",
    "BU_LC_INCREMENTAL_FEATURE_STRATEGY = [\n",
    "    LeafInitRule(),\n",
    "    FeatureEmptyPredictRule(),\n",
    "    FeatureBottomUpPredictCombineRule(),\n",
    "    FeatureCompleteFundamentalRule(),\n",
    "]\n",
    "\n",
    "\n",
    "class FeatureIncrementalChartParser(IncrementalChartParser, FeatureChartParser):\n",
    "    def __init__(\n",
    "        self,\n",
    "        grammar,\n",
    "        strategy=BU_LC_INCREMENTAL_FEATURE_STRATEGY,\n",
    "        trace_chart_width=20,\n",
    "        chart_class=FeatureIncrementalChart,\n",
    "        **parser_args\n",
    "    ):\n",
    "        IncrementalChartParser.__init__(\n",
    "            self,\n",
    "            grammar,\n",
    "            strategy=strategy,\n",
    "            trace_chart_width=trace_chart_width,\n",
    "            chart_class=chart_class,\n",
    "            **parser_args\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class FeatureEarleyChartParser(FeatureIncrementalChartParser):\n",
    "    def __init__(self, grammar, **parser_args):\n",
    "        FeatureIncrementalChartParser.__init__(\n",
    "            self, grammar, EARLEY_FEATURE_STRATEGY, **parser_args\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class FeatureIncrementalTopDownChartParser(FeatureIncrementalChartParser):\n",
    "    def __init__(self, grammar, **parser_args):\n",
    "        FeatureIncrementalChartParser.__init__(\n",
    "            self, grammar, TD_INCREMENTAL_FEATURE_STRATEGY, **parser_args\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class FeatureIncrementalBottomUpChartParser(FeatureIncrementalChartParser):\n",
    "    def __init__(self, grammar, **parser_args):\n",
    "        FeatureIncrementalChartParser.__init__(\n",
    "            self, grammar, BU_INCREMENTAL_FEATURE_STRATEGY, **parser_args\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class FeatureIncrementalBottomUpLeftCornerChartParser(FeatureIncrementalChartParser):\n",
    "    def __init__(self, grammar, **parser_args):\n",
    "        FeatureIncrementalChartParser.__init__(\n",
    "            self, grammar, BU_LC_INCREMENTAL_FEATURE_STRATEGY, **parser_args\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "# ////////////////////////////////////////////////////////////\n",
    "# Demonstration\n",
    "# ////////////////////////////////////////////////////////////\n",
    "\n",
    "\n",
    "def demo(\n",
    "    print_times=False,\n",
    "    print_grammar=False,\n",
    "    print_trees=True,\n",
    "    trace=2,\n",
    "    sent=\"John saw a dog\",\n",
    "    numparses=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    A demonstration of the Earley parsers.\n",
    "    \"\"\"\n",
    "    import sys, time\n",
    "    from nltk.parse.chart import demo_grammar\n",
    "    global tree\n",
    "    # The grammar for ChartParser and SteppingChartParser:\n",
    "    grammar = demo_grammar()\n",
    "    if print_grammar:\n",
    "        print(\"* Grammar\")\n",
    "        print(grammar)\n",
    "\n",
    "    # Tokenize the sample sentence.\n",
    "    print(\"* Sentence:\")\n",
    "    print(sent)\n",
    "    tokens = sent.split()\n",
    "    print(tokens)\n",
    "    print()\n",
    "\n",
    "    # Do the parsing.\n",
    "    earley = EarleyChartParser(grammar, trace=trace)\n",
    "    t = perf_counter()\n",
    "    chart = earley.chart_parse(tokens)\n",
    "    parses = list(chart.parses(grammar.start()))\n",
    "    t = perf_counter() - t\n",
    "\n",
    "    # Print results.\n",
    "    if numparses:\n",
    "        assert len(parses) == numparses, \"Not all parses found\"\n",
    "    if print_trees:\n",
    "        for tree in parses:\n",
    "            print(tree)\n",
    "    else:\n",
    "        print(\"Nr trees:\", len(parses))\n",
    "    if print_times:\n",
    "        print(\"Time:\", t)\n",
    "\n",
    "\n",
    "    print(\"----------------\")\n",
    "    if print_trees:\n",
    "        for tree in parses:\n",
    "            print(tree)\n",
    "    print(parses)\n",
    "    tree = parses[0]   \n",
    "if __name__ == \"__main__\":\n",
    "    demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  In the printTree method the input is an nltk.tree.Tree and the graph of the derivation tree is created a seris of using add_edge, label_edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(S',\n",
       " '(NP',\n",
       " 'John)',\n",
       " '(VP',\n",
       " '(Verb',\n",
       " 'saw)',\n",
       " '(NP',\n",
       " '(Det',\n",
       " 'a)',\n",
       " '(Noun',\n",
       " 'dog))))']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "from gvanim import Animation\n",
    "from gvanim.jupyter import interactive\n",
    "ga = Animation()\n",
    "def printTree(t):\n",
    "    if type(t) == nltk.tree.Tree:\n",
    "        nodes = (str(t)).split()\n",
    "        left = []\n",
    "        right = [] \n",
    "        removed = []\n",
    "        order = []\n",
    "        previous = 0 \n",
    "        for i in range(len(nodes)):\n",
    "            for j in range(len(nodes[i])):\n",
    "                if nodes[i][j] == '(':\n",
    "                    left.append(i)\n",
    "                if nodes[i][j] == ')':\n",
    "                    right.append(i)   \n",
    "\n",
    "        for i in range(len(nodes)):\n",
    "            for i in range(0,len(left)):\n",
    "                if (left[i] - left[i-1]) == 1:\n",
    "                    previous = left[i-1]\n",
    "                    current = left[i]\n",
    "                    #print(previous,current)\n",
    "                    ga.add_edge((nodes[previous]).replace('(','').replace(')',''), nodes[current].replace('(','').replace(')',''))\n",
    "                    order.append((previous,current))\n",
    "                elif (left[i] - left[i-1]) > 1:\n",
    "                    current = left[i]\n",
    "                    #print(previous,current)\n",
    "                    ga.add_edge(nodes[previous].replace('(','').replace(')',''), nodes[current].replace('(','').replace(')',''))\n",
    "                    order.append((previous,current))\n",
    "            for i in range(0,len(right)):\n",
    "                if (right[i]-1) in left:\n",
    "                    current = right[i] \n",
    "                    previous = right[i]-1\n",
    "                    #print(previous,current)\n",
    "                    ga.add_edge(nodes[previous].replace('(','').replace(')',''), nodes[current].replace('(','').replace(')',''))\n",
    "                    order.append((previous,current))\n",
    "                    left.remove(previous)       \n",
    "                elif (right[i-1])==(right[i]) and left !=[0] :\n",
    "                    left.append(right[i])\n",
    "                    left = sorted(left)\n",
    "                    close = (left.index(right[i])) - 1\n",
    "                    left.remove(right[i])\n",
    "                    #print(min(left),left[close])\n",
    "                    removed.append(left[close])\n",
    "                    if len(removed) >= 2 and (removed[0],removed[1]) in order:\n",
    "                        #print('removed:', removed[0], removed[1])\n",
    "                        ga.remove_edge(nodes[removed[0]].replace('(','').replace(')',''), nodes[removed[1]].replace('(','').replace(')',''))\n",
    "                        order.remove((removed[0],removed[1]))\n",
    "                    ga.add_edge(nodes[min(left)].replace('(','').replace(')',''), nodes[left[close]].replace('(','').replace(')',''))\n",
    "                    order.append((min(left),left[close]))\n",
    "                    left.remove(left[close])\n",
    "        return nodes\n",
    "    else:\n",
    "        raise Exception('error: incorrect input, expecting nltk.tree.Tree')\n",
    "printTree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(S',\n",
       " '(NP',\n",
       " 'John)',\n",
       " '(VP',\n",
       " '(Verb',\n",
       " 'saw)',\n",
       " '(NP',\n",
       " '(Det',\n",
       " 'a)',\n",
       " '(Noun',\n",
       " 'dog))))']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def animate(t):\n",
    "    if type(t) == nltk.tree.Tree:\n",
    "        nodes = printTree(t)\n",
    "        for i in nodes:\n",
    "            i = i.replace('(','').replace(')','')\n",
    "            ga.highlight_node(i)\n",
    "            ga.next_step()\n",
    "        return nodes\n",
    "    else:\n",
    "        raise Exception('error: incorrect input, expecting nltk.tree.Tree')\n",
    "    \n",
    "animate(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d046a2aa4a1421c86f878573c07c228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='n', max=11), Output()), _dom_classes=('widget-interact',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive( ga, 600 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Efficiency of Animation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time(s):  7.002453400000377\n",
      "Time(s):  1.0655314000005092\n",
      "Time(s):  1.4130919999997786\n",
      "Time(s):  1.602253500000188\n",
      "Time(s):  0.9936299999999392\n",
      "Average Runtime(s):  2.4153920600001584\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "interactive( ga, 600 )\n",
    "stop_time = timeit.default_timer()\n",
    "A = stop_time - start_time\n",
    "print('Time(s): ', A) \n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "interactive( ga, 600 )\n",
    "stop_time = timeit.default_timer()\n",
    "B = stop_time - start_time\n",
    "print('Time(s): ', B)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "interactive( ga, 600 )\n",
    "stop_time = timeit.default_timer()\n",
    "C = stop_time - start_time\n",
    "print('Time(s): ', C)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "interactive( ga, 600 )\n",
    "stop_time = timeit.default_timer()\n",
    "D = stop_time - start_time\n",
    "print('Time(s): ', D)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "interactive( ga, 600 )\n",
    "stop_time = timeit.default_timer()\n",
    "E = stop_time - start_time\n",
    "print('Time(s): ', E)\n",
    "\n",
    "average = (A + B + C + D + E)/5\n",
    "print('Average Runtime(s): ', average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
